{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Project 1: Urban/ Spatial Data Science - FYP 2021\n",
    "## Road Collision Analysis of Leeds (UK) \n",
    "---\n",
    "### Group 9: Aidan Stocks, Christian Margo Hansen, Jonas-Mika Senghaas, Malthe Pabst, Rasmus Bondo Hansen\n",
    "Submission: 25.02.2021 / Last Modified: 24.02.2021\n",
    "\n",
    "---\n",
    "\n",
    "This notebook contains the step-by-step data science process performed on the `Road Safety Data (UK)` from 2019. The goal was to inform the government of **Leeds** (Group 9 Focus) about traffic fatalities and injuries, to derive a future urban transport plan.\n",
    "\n",
    "The raw datasets was given by the course mananger. The datasets were downloaded from the following link on Jan 4th: https://data.gov.uk/dataset/road-accidents-safety-data. That page was updated afterwards (Jan 8th). Therefore local and online data may be inconsistent.\n",
    "\n",
    "> *Contact for technical difficulties or questions related to code: Jonas-Mika Senghaas (jsen@itu.dk)*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Introduction\n",
    "Experience shows that informed policy-making often translates into good policy-making. Especially for enhancing road safety, the use of data is critical to sustainably improve a city's urban mobility concept and reduce traffic-related casualties. In this notebook we will develop the code that was used to analyse the Road Safety in Leeds based on the extensive data that has been collected UK-wide in 2019.\n",
    "Later, the focus of this project (`TASK 4`) will be to make Leeds a bike-safer city. \n",
    "\n",
    "Leeds is the largest city in the county of West Yorkshire, England and the most populous in the Yorkshire and Humber region with roughly 800.000 inhabitants (2019, from *Eurostat*). Leeds' transport system is dominated by cars and it is currently the 9th most congested UK city. There have long been plans for a public transport network in Leeds, yet as of 2020, none have come to realisation, leaving Leeds' mobility landscape highly car-dominated."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Running this Notebook\n",
    "---\n",
    "This notebook contains all code to reproduce the findings of the project as can be seen on the [GitHub page](https://github.com/jonas-mika/fyp2021p01g09) of this project. In order to read in the data correctly, the global paths configured in the section `Constants` need to be correct. The following file structure - as prepared in the `submission.zip` - was followed throughout the project and is recommended to use (alternatively the paths in the section `Constants` can be adjusted):\n",
    "\n",
    "```\n",
    "submission\n",
    "|   github_link.webloc\n",
    "│   leeds_report.pdf\n",
    "│   gitlog.txt    \n",
    "│\n",
    "└───data\n",
    "│   │\n",
    "│   | raw\n",
    "│   | │   Road Safety Data - Accidents 2019.csv\n",
    "│   | │   Road Safety Data - Casualties 2019.csv\n",
    "│   | │   Road Safety Data- Vehicles 2019.csv\n",
    "|   |\n",
    "|   | references\n",
    "|     |   variable lookup.xls\n",
    "│   \n",
    "└───notebooks\n",
    "    |   project1.ipynb (current location)\n",
    "    |   project1\n",
    "        |   __init__.py\n",
    "        |   visualisations.py\n",
    "        |   ...\n",
    "    \n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Required Libraries\n",
    "---\n",
    "Throughout the project, we will use a range of both built-in and external Python Libraries. This notebook will only run if all libraries and modules are correctly installed on your local machines. \n",
    "To install missing packages use `pip install <package_name>` (PIP (Python Package Index) is the central package management system, read more [here](https://pypi.org/project/pip/)). \n",
    "\n",
    "In case you desire further information about the used packages, click the following links to find detailled documentations:\n",
    "- [Pandas Homepage](https://pandas.pydata.org/)\n",
    "- [Numpy Homepage](https://numpy.org/)\n",
    "- [Matplotlib Homepage](https://matplotlib.org/stable/index.html)\n",
    "- [Seaborn Homepage](https://seaborn.pydata.org/)\n",
    "- [Folium Documentation](https://python-visualization.github.io/folium/)\n",
    "- [Scipy Homepage](https://www.scipy.org/)\n",
    "- [Textwrap Documentation](https://docs.python.org/3/library/textwrap.html)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                                    # provides major datastructure pd.DataFrame() to store the datasets\n",
    "import numpy as np                                     # used for numerical calculations and fast array manipulations\n",
    "import matplotlib                                      # visualisation of data\n",
    "import matplotlib.pyplot as plt                        # visualisation of data\n",
    "import seaborn as sns                                  # plotting categorical vs. numerical variables\n",
    "import folium                                          # spatial visualisation\n",
    "from folium.plugins import HeatMap, MarkerCluster      # spatial visualisation\n",
    "from scipy.stats import chi2_contingency               # chi2 statistical association test\n",
    "import json                                            # data transfer to json format\n",
    "import os                                              # automates saving of export files (figures, summaries, ...)\n",
    "import random                                          # randomness in coloring of plots\n",
    "import textwrap                                        # prettify axis labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10) # we set the seed to get reproducable results"
   ]
  },
  {
   "source": [
    "## Own Package\n",
    "---\n",
    "Since this project makes heavy use of functions to achieve maximal efficiency, all functions are stored externally in the package structure `project1'. The following imports are necessary for this notebook to run properly."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project1.processing import check_indexes_in_subset, check_columns_for_missing_values\n",
    "from project1.numerical_summary import get_uniques_and_counts, get_fivenumsummary, compute_numerical_summary\n",
    "from project1.visualisations import initialise_summary, barplot, histogram, boxplot, categorical_scatterplot, categorical_association_test\n",
    "from project1.spatial_visualisation import plot_marker, random_color, map_accidents\n",
    "from project1.save import save_csv, save_figure, save_numerical_report, save_map, save_all_single_variable_analysis, save_all_categorical_scatters, save_all_categorical_associations"
   ]
  },
  {
   "source": [
    "**REMARK**: All function used in this project are well documented in their `Docstring`. To display the docstring and get an short summary of the function and the specifications of the input argument (including data tupe and small explanation) as well as their return value, type **`?<function_name>`** in Juptyer (*see example*)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Constants\n",
    "---\n",
    "To enhance the readibilty, as well as to decrease the maintenance effort, it is useful for bigger projects to define contants that need to be accessed globally throughout the whole notebook in advance. \n",
    "The following cell contains all of those global constants. By convention, we write them in caps (https://www.python.org/dev/peps/pep-0008/#constants)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path lookup dictionary to store the relative paths from the directory containing the jupyter notebooks to important directories in the project\n",
    "PATH = {}\n",
    "\n",
    "PATH['data'] = {}\n",
    "PATH['data']['raw'] = \"../data/raw/\"\n",
    "PATH['data']['interim'] = \"../data/interim/\"\n",
    "PATH['data']['processed'] = \"../data/processed/\"\n",
    "PATH['data']['external'] = \"../data/external/\"\n",
    "\n",
    "PATH[\"references\"] = \"../data/references/\"\n",
    "\n",
    "PATH['reports'] = {}\n",
    "PATH['reports']['leeds'] = '../reports/leeds/'\n",
    "PATH['reports']['bikes'] = '../reports/bikes/'\n",
    "PATH['reports']['external'] = '../reports/external/'\n",
    "\n",
    "PATH['accidents'] = 'accidents/'\n",
    "PATH['casualties'] = 'casualties/'\n",
    "PATH['vehicles'] = 'vehicles/'\n",
    "\n",
    "\n",
    "# filename lookup dictionary storing the most relevant filenames\n",
    "FILENAME = {}\n",
    "FILENAME['accidents'] = 'Road Safety Data - Accidents 2019.csv'\n",
    "FILENAME['casualties'] = 'Road Safety Data - Casualties 2019.csv'\n",
    "FILENAME['vehicles'] = 'Road Safety Data- Vehicles 2019.csv' # the original dataset has a small typing mistake\n",
    "FILENAME['variable_lookup'] = 'variable lookup.xls'\n",
    "FILENAME['road_flow'] = 'dft_rawcount_local_authority_id_63.csv'\n",
    "\n",
    "# list of internal names for datasets (used to easily iterate over them)\n",
    "TABLENAMES = [\"accidents\", \"casualties\", \"vehicles\"]\n",
    "\n",
    "# defining three dictionaries to store data. each dictionary will reference a pandas dataframe to a string-representation as a key.\n",
    "DATA_RAW = {}\n",
    "DATA_LEEDS = {}\n",
    "DATA_EXTERNAL = {}\n",
    "DATA_LEEDS_BIKES = {}\n",
    "\n",
    "# for automatic labelling of the plots, we make use of an external excel-sheet that was provided with the data. we will read in the lookup of the variables into the following dictionary\n",
    "VARIABLE_LOOKUP = {}\n",
    "\n",
    "# automising all plots requires a lot of additional information (ie. what plot-type to use on the different variables, whether or not we need a fivenumber-summary, etc.). this information is stored in the summary dictionary\n",
    "SUMMARY = {}\n",
    "SUMMARY_BIKES = {}"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Loading, Inspection and Processing of Datasets (TASK 0)\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Loading in the Datasets\n",
    "---\n",
    "Every data analysis requires a dataset. Therefore the first step of every data science project is to load in the data that was provided and on which the data analysis should be based.\n",
    "We conveniently load in all three datasets into an individual Pandas `DataFrame` using the built-in pandas method `pd.read_csv()` ([Documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)). We store those in our `DATA_RAW` dictionary in the corresponding keys."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all three datasets using pandas into the predefined dictionary 'DATA_RAW' where the key corresponds to the internal dataset name\n",
    "for dataset in TABLENAMES:\n",
    "    DATA_RAW[dataset] = pd.read_csv(PATH['data']['raw'] + FILENAME[dataset])"
   ]
  },
  {
   "source": [
    "## Inspection of Datasets\n",
    "---\n",
    "We can now have a look at our three dataset to get a good first impression for what kind of data we are dealing with. We start by reporting the number of records and fields/  variables in each of the datasets by using the `shape` property of the Pandas `DataFrame`. \n",
    "We then continue to have an actual look into the data. Similiar to the `head` command in terminal, we can use the method `head()` onto our `DataFrames`, which outputs a nice inline representation of the first five data records of the dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Size (Number of Records and Fields)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    print(f\"{dataset.capitalize()}: {DATA_RAW[dataset].shape}\")"
   ]
  },
  {
   "source": [
    "### Peek into Datasets (Describing Attributes)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_RAW['accidents'].head()"
   ]
  },
  {
   "source": [
    "We see, that the main dataset `accidents` stores all recorded accidents in 2019. It consist of 117.536 rows, corresponding to single reported accidents, that is identified with a unique index (in the first column). Each accident is descibed via 31 descriptive variables, providing more detailed information about the accident. \n",
    "The following overview classifies all 32 variables in the dataset. It is important to differentiate the variables as their type determines how we will treat them in the further analysis:\n",
    "\n",
    ">**Categorical Attributes**\n",
    ">> `Accident_Severity (6)`, `Number_of_Vehicles (7)`, `Number_of_Casualties (8)`, `1st_Road_Class (14)`, `2st_Road_Number (15)`, `Road_Type (16)`, `Speed_limit (17)`, `Junction_Detail (18)`, `Junction_Control (19)`, `2nd_Road_Class (20)`, `2nd_Road_Number (21)`, `Pedestrian_Crossing-Human_Control (22)`, `Pedestrian_Crossing-Physical_Facilities (23)`, `Light_Conditions (24)`, `Weather_Conditions (25)`, `Road_Surface_Conditions (26)`, `Special_Conditions_at_Site (27)`, `Carriageway_Hazards (28)`, `Urban_or_Rural_Area (29)`, `Did_Police_Officer_Attend_Scene_of_Accident (30)`\n",
    "\n",
    ">**Geographical Attributes** (There are several measures of the location of the accident)\n",
    ">> `Location_Easting_OSGR (1)`, `Location_Northing_OSGR (2)`, `Longitude (3)`, `Latitude (4)`, `Police Force (5)`, `Local_Authority_(District) (12)`, `Local_Authority_(Highway) (13)`, `LSOA_of_Accident_Location (31)`\n",
    "\n",
    ">**Time Attributes** \n",
    ">> `Date (9)`, `Time (11)`, `Day_of_Week (10)`\n",
    "\n",
    ">**Relational Attributes**\n",
    ">> `Accident_Index (0)`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_RAW['casualties'].head()"
   ]
  },
  {
   "source": [
    "We see, that the sub-dataset `casualties` provides more detailed information about the casualties of all accidents. It consist of 153.158 columns (which leads to 153.158 records on casualties) and has 16 columns providing more detailed information about the casualty. Again, we inspect all column attributes to get a feel for the information the dataset stores. We can classify as follows:\n",
    ">**Categorical Attributes**\n",
    ">> `Casualty_Class (4)`, `Sex_of_Casualty (5)`, `Age_Band_of_Casualty (7)`, `Casualty_Severity (8)`, `Pedestrian_Location (9)`, `Car_Passenger (10)`, `Bus_or_Coach_Passenger (11)`, `Pedstrian_Road_Maintenace_Worker (12)`, `Casualty_Type (13)`, `Casualty_Home_Area_Type (14)`, `Casualy_IMD_Decile (15)`\n",
    "\n",
    ">**Numerical Attributes**\n",
    ">> `Age_of_Casualty (6)`\n",
    "\n",
    ">**Relational Attributes**\n",
    ">> `Accident_Index (0)`, `Vehicle_Reference (1)`, `Casualty_Refernce (2)`, "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_RAW['vehicles'].head()"
   ]
  },
  {
   "source": [
    "We see, that the sub-dataset `vehicles` provides more detailed information about the vehicles of all accidents. It consist of 216.381 columns (which leads to 216.381 records on casualties) and has 16 columns providing more detailed information about the casualty. Again, we inspect all column attributes to get a feel for the information the dataset stores. We can classify as follows:\n",
    ">**Categorical Attributes**\n",
    ">> `Vehicle_Type (2)`, `Towing_and_Articulation (3)`, `Vehicle_Manoeuvre (4)`, `Vehicle_Location_Restricted_Lane (5)`, `Junction_Location (6)`, `Skidding_and_Overturning (7)`, `Hit_Object_in_Carriageway (8)`, `Vehicle_Leaving_Carriageway (9)`, `Hit_Object_off_Carriageway (10)`, `1st_Point_of_Impact (11)`, `Was_Vehicle_Left_Hand_Drive? (12)`, `Journey_Purpose_of_Driver (13)`, `Sex_of_Driver (14)`, `Age_Band_of_Driver (16)`, `Propulsion_Code (18)`, `Driver_IMD_Decile (19)`, `Driver_Home_Area_Type (20)`, `Vehicle_IMD_Decile (21)`\n",
    "\n",
    ">**Numerical Attributes**\n",
    ">> `Age_of_Casualty (6)`, `Age_of_Driver (15)`, `Engine_Capacity_(CC) (17)`, \n",
    "\n",
    ">**Relational Attributes**\n",
    ">> `Accident_Index (0)`, `Vehicle_Reference (1)`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Summary\n",
    "We see that there exist three different datasets, that all hold information about the road collisions reported in the UK in 2019. Together the three datasets contain very rich and detailed information, allowing to theoretically reconstruct every reported accident. It is important for us to recognize that the number of records in the two subdatasets `casualties` and `vehicles` is greater than the number of records in the `accidents` dataset, since an accident involves at least one vehicle and often more than one casualty.\n",
    "\n",
    "We can link the casualties and vehicles involved in an accident by their `Accident_Index`.\n",
    "Likewise, we can connect the casualties to the vehicles using the `Vehicle_Reference` attribute."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Initial Sanity Check\n",
    "---\n",
    "Before continuing with the data analysis, we want to make sure that the dataset is clean. There are a plentiful of methods to check this. In the following, we will stick to the following three:\n",
    ">(a) **Uniqueness of Indexes** (*Are there multiple indexes in `accidents`?*)\n",
    "\n",
    ">(b) **Linking Behavior** (*Are there indexes in the subdatasets that do not occur in `accidents`?*)\n",
    "\n",
    ">(c) **Missing Values** (*Are there missing values? If yes, how many and in which columns?*)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Uniqueness of Indexes\n",
    "We are first evaluating if there are mulitple indexes in the main dataset `accidents`. This is a first, very basic metric to determine whether the data in the dataset has been inputted correctly. In the case of three interlaced datasets, this check is even more important, since the `Accident_Index' property links the main dataset to the two sub-datasets.\n",
    "\n",
    "The condition of unique indexes holds, if and only if the number unique elements in the first column (`Accident_Index`) is equal to the total number of records in the dataset. We check this condition as follows:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we check if there are mulitple indexes in the accidents dataset\n",
    "DATA_RAW['accidents'].shape[0] == len(set(DATA_RAW['accidents']['Accident_Index']))"
   ]
  },
  {
   "source": [
    "Perfect. There do not seem to be any multiple indexes. Each row in the dataset `accidents` seems to refer to a unique accident, that we can use reference in the two sub-datasets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Linking Behavior\n",
    "Another index metric evolves around the `Accident_Indexes` in the two sub-datasets. Ideally, no indexes that are not in the main dataset `accidents` appear in the two sub-datasets, since this would mean that there exist information on vehicles and casualties that are referenced by an unknown accident. \n",
    "\n",
    "We check for this condition using our own function `check_indexes_in_subset` that takes in the two columns containing the `Accident_Index`es in both the sub and main dataset and return either `None` if all indexes are correct or the number of wrong indexes if there are mistakes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the wrong indexes for each sub dataset\n",
    "for dataset in TABLENAMES:\n",
    "    if dataset != 'accidents':\n",
    "        print(f\"Number of Missing Indexes in {dataset.capitalize()}: {check_indexes_in_subset(DATA_RAW[dataset]['Accident_Index'], DATA_RAW['accidents']['Accident_Index'])}\")"
   ]
  },
  {
   "source": [
    "This is rather bad. Roughly 21.500 indexes in the raw `vehicles` dataset are linking to an accident that is not registered in the `accidents` dataset. 19.300 in the `casualties` dataset link casualties to accidents that are not registered in the `accidents` dataset. \n",
    "If we observe a similar behavior for our filted dataset for Leeds, we would need to investigate how those wrong indexes appear as they can have a major impact of the trustworthiness of the data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Missing Values\n",
    "Lastly, we check in all columns in all datasets the number of missing values (encoded as `' '` (*empty strings*)) to see if there are missing values (empty string) to get a feeling on which columns we need to further do processing."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    print(dataset.capitalize())\n",
    "    check_columns_for_missing_values(DATA_RAW[dataset])\n",
    "    print('\\n')"
   ]
  },
  {
   "source": [
    "That's not bad! There are no missing values in both sub datasets. \n",
    "It seems like in the accidents dataset, there are 28 accidents that have no information on their location. This only gets important for our analysis if one of those accidents is located in Leeds, then we would need to deal with this issue later. The LSOA Metric - which is another measure of the accident location of the accident - hasn't been registered for 5714 accidents. This is not important for our analysis, since we will use the longitude and latitude to plot the accidents' location. \n",
    "There are, however, 63 accidents for which the time of accident is not registered. If any of those accidents are located in Leeds, we have to deal with them later."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Processing \n",
    "---\n",
    "We now know a lot more about our datasets. We have inspected each dataset's size (ie. the number of records and fields), and have inspected and classified each of the attributes in the datasets. We also generally proved the dataset to be clean.\n",
    "\n",
    "This clears the path to filter all three datasets to only hold information about the city of interest: `Leeds`. There are several ways to do this. We could ie. filter by an geographic attribue (as listed in the `Description of Attributes`). However, there is a faster way to achieve the exact same result, since the `accidents` database contains an informative attribute called `Local_Authority_(District)`. \n",
    "> The Local Authority Districts, also known as the [Districts of England](https://en.wikipedia.org/wiki/Districts_of_England) is a subnational division of the UK into districts, each identified with a unique number. Learn more about the clustering of the country [here](https://en.wikipedia.org/wiki/Subdivisions_of_England). Note, that ie. the attribute LSOA Location is another metric to geographically identify places in the UK, but in a more precise fashion. For the purpose of filtering for a whole city as Leeds, we however don't need this level of precision, but can make use of the fact that Leeds is a local authority district. \n",
    "\n",
    "From the `variable lookup.xls` sheet (which contains information about the categorical variables in all three datasets), we learn that the code `204` relates to `Leeds`. \n",
    "With this knowledge we can conveniently filter the main dataset `accidents` for `Leeds`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS[\"accidents\"] = DATA_RAW['accidents'][DATA_RAW['accidents']['Local_Authority_(District)'] == 204]"
   ]
  },
  {
   "source": [
    "However, the two sub-datasets cannont be identified by the `Local_Authority_(District)`, since they both do not contain this attribute. \n",
    "Therefore, we need to filter for all unique accidents that occurred in Leeds and extract the `Accident Indexes`. With this relational attribute - a list of accident indexes in Leeds, we can extract the vehicles and casualties related to these Leeds accidents. but need to be filtered through the unique accident indexes that we can obtain from our filtered dataframe of accidents in 'Leeds'. We obtain a list of all accident indexes of the accidents that occured in Leeds and use this index list to filter both the 'vehicles.csv' and 'casualties.csv' datasets.\n",
    "Therefore, we need to filter for all unique accidents that occurred in Leeds and extract the `Accident Indexes`. With this relational attribute - a list of accident indexes in Leeds, we can extract the vehicles and casualties related to these Leeds accidents. \n",
    "\n",
    "We do this conveniently with Pandas `.isin()` method on a dataframe to create a mask that filter out all accident indexes not related to Leeds. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leeds_indexes = list(DATA_LEEDS['accidents']['Accident_Index']) # we dont need to set() this because we know that all indexes are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS[\"casualties\"] = DATA_RAW['casualties'][DATA_RAW['casualties']['Accident_Index'].isin(leeds_indexes)]\n",
    "DATA_LEEDS[\"vehicles\"] = DATA_RAW['vehicles'][DATA_RAW['vehicles']['Accident_Index'].isin(leeds_indexes)]"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Inspection\n",
    "---\n",
    "With our newly filtered datasets, we have reduced the number of records about accidents, casualties and vehicles in the datasets `accidents`, `casualties` and `vehicles`, respectively. To see, with what kind of data we are dealing now, it makes to recompute the size of each of the datasets. However, our analysis of the column attributes still holds, since we have only filtered across `axis=0` (the rows), while we left the attributes untouched. \n",
    "\n",
    "Again, we report the number of records and fields/ variables in each of the datasets by using the `shape` property of the Pandas `DataFrame`. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    print(f\"{dataset.capitalize()}: {DATA_LEEDS[dataset].shape}\")"
   ]
  },
  {
   "source": [
    "The datasets are tremendously reduced. From 117.536 reported accidents in the whole of UK, 1.451 occurred in Leeds. In those 1.451 accidents, 1.908 were affected and 2.688 vehicles were involved."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Saving Filtered Data\n",
    "---\n",
    "After having filtered the raw datasets into a filted one, that only contains records about accidents located in Leeds, we want to save this interims version of the data as a `csv` into an interim folder. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    save_csv(DATA_LEEDS[dataset], path=PATH['data']['interim'], filename=FILENAME[dataset][:-4], index=False) "
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Sanity Check for Leeds\n",
    "--- \n",
    "With the filtered dataset, we want to repeat our initial sanity checks. This is necessary to know, at which point we need to further process the dataset. We check for the following (identical) conditions:\n",
    ">(a) **Uniqueness of Indexes** (*Are there multiple indexes in `accidents`?*)\n",
    "\n",
    ">(b) **Linking Behavior** (*Are there indexes in the subdatasets that do not occur in `accidents`?*)\n",
    "\n",
    ">(c) **Missing Values** (*Are there missing values? If yes, how many and in which columns?*)\n",
    "\n",
    "*For more detailed information about the functions used, revisit the section `Initial Sanity Check` or type `?<function_name>` in an individual cell to obtain a small, but informative description (docstring) about each function.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Uniqueness of Indexes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "DATA_LEEDS['accidents'].shape[0] == len(set(DATA_LEEDS['accidents']['Accident_Index']))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### Linking Behavior"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    if dataset != 'accidents':\n",
    "        print(f\"#Missing indexes in {dataset.capitalize()}: {check_indexes_in_subset(DATA_LEEDS[dataset]['Accident_Index'], DATA_LEEDS['accidents']['Accident_Index'])}\")"
   ]
  },
  {
   "source": [
    "### Missing Values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    print(dataset.capitalize())\n",
    "    check_columns_for_missing_values(DATA_LEEDS[dataset])\n",
    "    print('\\n')"
   ]
  },
  {
   "source": [
    "Perfect. None of the sanity checks report any problems on our dataset. At this point we could export the dataset and start analysing the data. However, we are making some adjustments in the below section to make our analysis easier."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Process Data\n",
    "---\n",
    "Luckily, the data is already quite clean. Except for 11 out of the total of 1.451 accidents, that did not register the time of the accident, all records in all three datasets are complete. Furthermore, the linking behavior between the three datasets seems legit. \n",
    "Before starting our analysis, we want to make slight changes to our data, in order to have an easier life later in the analysis."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### `Accidents`: Time (*Column: 11*)\n",
    "Our sanity check on the `Leeds` data has reported 11 missing time values. In order to not crash due to weird behaviors of empty strings, we want to clean this now. Therefore we change all empty strings to the string `-1`. This behavior is chosen by convention and is also the standard for the given dataset, as can be observed in the `variable lookup.cls`, where `Missing Values`/`N/A` are identified as `-1`.\n",
    "Since we later also want to report the distributin of accident frequency during different times of the day, we slice the `Time`-String, such that it only holds the hour of the accident. This level of detail is sufficient for our analysis and simplifies the further process.\n",
    "\n",
    "The following block of code implements the above described behavior:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.array(DATA_LEEDS['accidents']['Time'])\n",
    "for i in range(len(time)):\n",
    "    try: \n",
    "        time[i] = int(time[i][:2])\n",
    "    except:\n",
    "        time[i] = -1\n",
    "\n",
    "DATA_LEEDS['accidents']['Time'] = time"
   ]
  },
  {
   "source": [
    "### `Accidents`: Date (*Column: 10*)\n",
    "To make meaningful plots, we also want to adjust the data in the `Date` column in the `accidents` dataset. The column gives dates in the format `DD/MM/YYYY`. Since all accidents report accidents from 2019, we can neglect this parameter. Furthermore, the specific day is a too specific time frame to be meaningful in an analysis. We there slice the `Date`-String, such that it only holds the month, in which the accident occurred. This level of detail is sufficient for our analysis and simplifies the further process."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = np.array(DATA_LEEDS['accidents']['Date'])\n",
    "\n",
    "for i in range(len(date)):\n",
    "    date[i] = int(date[i][3:5])\n",
    "\n",
    "DATA_LEEDS['accidents']['Date'] = date"
   ]
  },
  {
   "source": [
    "### `Accidents`: 2nd Road Class (*Column: 20*)\n",
    "Inspecting both the data and the variable lookup, we noticed a minor - but not unimportant - discrepancy in the 20th column of the `accidents` dataset. From the variable lookup, the following lookup is expected {0: Not at junction or within 20 metres, ...}. However, in the dataset, there do not appear any `0`, but a lot of `-1` (which does not appear in the variable lookup for this column). It is therefore reasonable to argue that this is a mistake in the variable lookup (and could thusly be easily fixed, by exchanging the `0` for a `-1`). In this example however, we decided to change all `-1` to `0` in the column, in order to make the jupyter work on the original files given.\n",
    "\n",
    "The following block of code implements this change."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_road_class = np.array(DATA_LEEDS['accidents']['2nd_Road_Class'])\n",
    "\n",
    "for i in range(len(second_road_class)):\n",
    "    if second_road_class[i] == -1:\n",
    "        second_road_class[i] = 0\n",
    "\n",
    "DATA_LEEDS['accidents']['2nd_Road_Class'] = second_road_class"
   ]
  },
  {
   "source": [
    "## Export Processed Datasets\n",
    "--- \n",
    "Finally, we export the processed datasets into a new subfolder. From now on, all Jupyter Notebooks will work with those processed datasets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    save_csv(DATA_LEEDS[dataset], path=PATH['data']['processed'], filename=FILENAME[dataset][:-4], index=False)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Single Variable Analysis (TASK 1)\n",
    "---\n",
    "We have obtained filtered and processed datasets from TASK 0. We will now turn our focus to analysing each attribute in each of the datasets both in a numerical summary (ie. through the number of uniques, counts of uniques or a five-number summary, where it makes sense) and visual representation. Depending on the type of the variable, there arouse different preferred ways of visual repsentation, to get a visual feeling for the data we are dealing with. \n",
    "> `Barplot` (*Categorical Variables*)\n",
    ">> We usually want to plot categorical variables in a bar plot, where the x-axis is labelled with the unique values measured in the specific column and the y-axis represents the number of occurences of each unique value. An example of a bar plot in this project is ie. the Number of Accidents per Week Day.\n",
    "\n",
    "> `Histogram` (*Numerical Variables*)\n",
    ">> Most numerical variables (especially those with a wide range), are plotted in a histogram (which accounts for the fact that numerical variables are continuous). An example of a histogram is ie. the Age Distribution of Casualties involved in an Accident.\n",
    "\n",
    "> `None` (*Others*)\n",
    ">> For some variables it doesn't make sense to plot them, ie. geographical (`Longitude`, ...) or relatinonal attributes (`Accident_Index`, ...)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Importing Variable Lookup \n",
    "---\n",
    "Before calculating the numerical summaries for each column and visualising them, we import the `variable lookup.xls` from the given references. The Excel-Sheet encodes which numerical value corresponds to which meaning for each column. This is essential for the dataset to be meaningful (otherwise it is just a bunch of useless numbers). Likewise, our plots should contain the human-readable interpretation of the variables, not the numerical values, to make them readable to everyone (even for those not having the `variable lookup`).\n",
    "\n",
    "In order to save a lot of work by manually copy/pasting from the Excel-Sheet, we use pandas built-in method `pd.ExcelFile` that instantiates a `ExcelFile` object, containing a lot of useful functionality to conveniently read in the lookups. In the following block of code we create a dictionary, where each key corresponds to the index of the excel sheet and leads to nested dictionary that holds the correct encoding of the column obtained from the `variable lookup`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xls = pd.ExcelFile(PATH['references'] + FILENAME['variable_lookup']) # instatiate ExcelFile object using pandas built-in `pd.ExcelFile`\n",
    "\n",
    "# read in excel data into python dict of dicts\n",
    "excel_dict = {i: xls.parse(xls.sheet_names[i]).to_dict() for i in range(len(xls.sheet_names))}\n",
    "\n",
    "# create convenient lookup dictionary for all excel sheets\n",
    "for x in range(2, len(excel_dict)): # start from sheet with index 2 (since previous do not encode lookups, and are therefore formatted differently)\n",
    "    VARIABLE_LOOKUP[x] = {} # create key-value pair with key being the index of the sheet and value an empty dictionary\n",
    "    try:\n",
    "        for i in range(len(excel_dict[x]['code'])):\n",
    "            VARIABLE_LOOKUP[x][excel_dict[x]['code'][i]] = excel_dict[x]['label'][i] # merge `code` dict and `label` dict \n",
    "    except: # account for irregularity in the excel sheet at sheet index 6, where the column names are capitalised (unlike all other sheets)\n",
    "        for i in range(len(excel_dict[x]['Code'])):\n",
    "            VARIABLE_LOOKUP[x][excel_dict[x]['Code'][i]] = excel_dict[x]['Label'][i] # merge `code` dict and `label` dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# account for irregularity in excel sheet at index 36 (Vehicle Propulsion Code)\n",
    "del VARIABLE_LOOKUP[36][' M']\n",
    "VARIABLE_LOOKUP[36][-1] = 'Undefined'"
   ]
  },
  {
   "source": [
    "## Initialise Summaries \n",
    "---\n",
    "In order to automate the process of creating numerical summaries and appropriate visualisations for each plot, we create a central datastructure for each of the datasets, which we call `SUMMARY` (*see this in `Constants`). The `SUMMARY` datastructure is a rich datastructure defined for each dataset,which initially contains the name, label behavior (*Does the column have a Lookup?*), plotting behavior (*How do we want to plot the column?*) and information about the five-number summary (*Whether or not we want to compute a five-number summary?*). \n",
    "Later we will fill this data structure even further and use is as a basis for all our plots. \n",
    "\n",
    "However, the first step is to manually inspect all columns and decide on how we want to deal with them. We hardcode these observation into three individually dictionaries defined for each dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary holding the column indexes in the specific dataset, which have a variable lookup\n",
    "LABELS = {}\n",
    "LABELS['accidents'] = [5, 6, 10, 12, 13, 14, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
    "LABELS['casualties'] = [3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "LABELS['vehicles'] = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 18]\n",
    "\n",
    "# dictinary that for each column in each dataset specifies the type of plot to use\n",
    "PLOTTING = {}\n",
    "PLOTTING['accidents'] = [None, None, None, None, None, 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', None]\n",
    "PLOTTING['casualties'] = [None, None, None, 'bar', 'bar', 'hist', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar',None]\n",
    "PLOTTING['vehicles'] = [None, None, 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'hist', 'bar', 'hist', 'bar', 'hist', None, 'bar', None]\n",
    "\n",
    "# dictionary holding the column indexes in the specific dataset, for which it makes sense to compute a five-number summary\n",
    "FIVENUM = {}\n",
    "FIVENUM['accidents'] = [6, 7, 8, 17]\n",
    "FIVENUM['casualties'] = [5]\n",
    "FIVENUM['vehicles'] = [15, 17, 19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, start_at in zip(TABLENAMES, [2, 37, 22]): # start_at corresponds to sheet indexes at which it switches to new dataset\n",
    "    initialise_summary(\n",
    "        data = DATA_LEEDS[dataset], \n",
    "        lookup = VARIABLE_LOOKUP, \n",
    "        dataset_name = dataset, \n",
    "        key = dataset, \n",
    "        summary = SUMMARY, \n",
    "        labels = LABELS[dataset], \n",
    "        plotting = PLOTTING[dataset], \n",
    "        fivenum = FIVENUM[dataset], \n",
    "        start_at=start_at)"
   ]
  },
  {
   "source": [
    "We also want to have a nice mapping for the time, date and the number of vehicles and casualties. We do this manually, since it is not defined in the given variable lookup excel."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time mapping\n",
    "SUMMARY['accidents'][11]['Map'] = {i: f\"{i}-{i+1}\" for i in range(24)}\n",
    "SUMMARY['accidents'][11]['Map'][-1] = 'N/A'\n",
    "\n",
    "# date mapping\n",
    "months = ['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC']\n",
    "SUMMARY['accidents'][9]['Map'] = {i+1: months[i] for i in range(12)}\n",
    "\n",
    "# no. casualties and vehicles\n",
    "SUMMARY['accidents'][7]['Map'] = {i: i for i in np.unique(DATA_LEEDS['accidents']['Number_of_Vehicles'])}\n",
    "SUMMARY['accidents'][8]['Map'] = {i: i for i in np.unique(DATA_LEEDS['accidents']['Number_of_Casualties'])}\n",
    "\n",
    "# fix windows issue with naming of attribute `Was_Vehicle_Left_Hand_Drive?`\n",
    "SUMMARY['vehicles'][12]['Name'] = 'Was_Vehicle_Left_Hand_Drive'"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Numerical Analysis\n",
    "---\n",
    "We have defined our central datastructure `SUMMARY` that currenlty contains information on how to process each column in each dataset. Given the column index as a key it stores...\n",
    "> ...the name of the attribute at `['Name']`\n",
    "\n",
    "> ...the way we would like to plot the attribute at `['Plot']`\n",
    "\n",
    "> ...whether or not we need a five-number summary at `['Summary']`\n",
    "\n",
    "> ...the mapping from the variable lookup (if there exist one) at `['Map']`\n",
    "\n",
    "However, the dictionary does not yet contain any data or actual summaries. We will change this in the following section. With the help of the small helper-function `get_uniques_and_counts` and `get_fivenumsummary`, we can easily iterate over the summaries of each dataset in `compute_numerical_summary` and depending on the specified properties of each column, make computations. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    compute_numerical_summary(SUMMARY[dataset], DATA_LEEDS[dataset])"
   ]
  },
  {
   "source": [
    "Great! For each dataset, we have computed the relevant data in each column. Depending on the properties, the following properties have been added...\n",
    "\n",
    "> ...the number of uniques values at `['No_Uniques']` for all columns (independent of their type)\n",
    "\n",
    "> ...a five-number summary at `['Five_Number_Summary']` if it is meaningful to compute a five-number summary for the column (ie. `['Summary']`==True)\n",
    "\n",
    "> ...a dictionary of uniques and counts at `['Uniques']` if the column should be visualised as a barplot (ie. `['Plot']`=='bar')\n",
    "\n",
    "> ...a data property storing all values in the column at `['Data']` if the column should be visualised as a histogram or barplot (ie. `['Plot']`=='hist')"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Saving Numerical Reports\n",
    "We have now computed all relevant data necessary to plot our data. Before doing so, we want to save the numerical summaries using our function `save_numerical_report`. The function takes in the `SUMMARY` datastructures and saves it either as a `csv` or `json` into the specified path for further inspection and external reports. The ouput shows into which directory the files were saved."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    save_numerical_report(SUMMARY[dataset], path=PATH['reports']['leeds'] + PATH[dataset] + 'numerical_summary/', filename=dataset,save_to='csv')\n",
    "    save_numerical_report(SUMMARY[dataset], path=PATH['reports']['leeds'] + PATH[dataset] + 'numerical_summary/', filename=dataset,save_to='json')"
   ]
  },
  {
   "source": [
    "## Visualisation\n",
    "---\n",
    "An essential step of each data exploration is to visualise our data. Only this makes the abstract numbers and characters meaningful.\n",
    "\n",
    "We are therefore visualising the results of the single variable analysis in this section using the data structure `SUMMARY` we have step-by-step developed throughout the project. Depending on the properties of the attribute to plot, there arise three types of visualisatin:\n",
    "> `Barplot`: The standard way of representing `categorical variables` using rectangular bars with heights and widths ([More information](https://en.wikipedia.org/wiki/Bar_chart))\n",
    "\n",
    "> `Histogram`: A histogram is an approximate representation of the distribution of numerical data ([More information](https://en.wikipedia.org/wiki/Histogram))\n",
    "\n",
    "> `Boxplot`: Boxplots are an important statistical tool to represent five-number summaries of numerical data visually ([More information](https://en.wikipedia.org/wiki/Box_plot))\n",
    "\n",
    "To efficiently plot everything, we defined three unique functions `barplot`, `histogram` and `boxplot` that all take in the `SUMMARY` data structure as their main argument and 'intelligently' combine all informatation we have previously gathered to give a nice visualisation of the data.\n",
    "We use the function `save_all_single_variable_analysis` that depends on `save_figure` to iterate over all columns and for each attributes computes and saves the correct plot in the correct directory. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Saving Figures\n",
    "---\n",
    "For our report, we want to have nice visualisations for all our figures. To achieve this, we use the helper-function `save_figure` that depends on the property `.savefig()` of `maplotlib.pyplot.Figure` objects. We call this function on all the plots of our datasets, which automatically saves all figures into the correct directories. When executed with the path specified below, the figures are in the following directory:\n",
    "> `../reports/leeds/accidents/single_variable_analysis`\n",
    "\n",
    "> `../reports/leeds/casualties/single_variable_analysis`\n",
    "\n",
    "> `../reports/leeds/vehicles/single_variable_analysis`\n",
    "\n",
    "**REMARK**: Be aware that the cell takes some time to compute, since it plots and saves all figures at once."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "for dataset in TABLENAMES:\n",
    "    save_all_single_variable_analysis(SUMMARY[dataset], path=PATH['reports']['leeds'] + PATH[dataset] + 'single_variable_analysis/', missing_values=True)\n",
    "    save_all_single_variable_analysis(SUMMARY[dataset], path=PATH['reports']['leeds'] + PATH[dataset] + 'single_variable_analysis/', missing_values=False)"
   ]
  },
  {
   "source": [
    "## Reports of Single Variable Analysis\n",
    "---\n",
    "`TASK 1` specifically asks us to report the frequency of accidents...\n",
    "> a. ...in different age groups\n",
    "\n",
    "> b. ...in different times of the day, week or year\n",
    "\n",
    "> c. ...in one other condition\n",
    "\n",
    "The following reports (that can also be found in the automatically generated plots in `reports/leeds/`) answer these questions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Accident Frequency for different age groups"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_per_age_band = barplot(SUMMARY['vehicles'][16],  keep_missing_values=False) # age band\n",
    "accidents_per_age = histogram(SUMMARY['vehicles'][15], keep_missing_values=False) # age"
   ]
  },
  {
   "source": [
    "### Accident Frequency for different times of the day, week and year"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_per_hour_of_day = barplot(SUMMARY['accidents'][11], keep_missing_values=False) # time\n",
    "accidents_per_day_of_week = barplot(SUMMARY['accidents'][10], keep_missing_values=False) # day of week\n",
    "accidents_per_month_of_year = barplot(SUMMARY['accidents'][9], keep_missing_values=False) # date"
   ]
  },
  {
   "source": [
    "### Accident Frequency for different Environmental Conditions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_by_light_condition = barplot(SUMMARY['accidents'][24], keep_missing_values=False)\n",
    "accidents_by_weather_condition = barplot(SUMMARY['accidents'][25], keep_missing_values=False)\n",
    "accidents_by_road_surface_condition = barplot(SUMMARY['accidents'][26], keep_missing_values=False)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Associations (TASK 2)\n",
    "---\n",
    "An equally important, and perhaps more challenging, part of any data analysis is to find associations in a dataset. Especially when analysing *Road Safety* this is a key aspect of the analysis, as we can investigate which accident, casualty or vehicle attribute lead to more frequent or severe accidents. These insights, in turn, allow us to proactively counteract through political measures. \n",
    "\n",
    "Since our dataset is dominated by `categorical variables` (as seen in the `Initial Inspection of Datasets`), we mostly want to relate `categorical variables` to each other and the few `numerical variables` that exist there. The following methods of associating are used in the following section to investigate these associations:\n",
    "> **`Categorical/ Numerical`**\n",
    ">> For relating a categorical to a numerical variable we use so-called `categorical scatterplots`, which are a special kind of scatterplot which displays the distribution of a numerical variable for different attributes in the cateogrical variable. To plot these kind of plots we use our own function `categorical_scatterplot` that at its core depends on `sns.catplot` ([Documentation](https://seaborn.pydata.org/generated/seaborn.catplot.html))\n",
    "\n",
    "> **`Categorical/ Categorical`**\n",
    ">> Relating two categorical attributes to one another is slightly more difficult. To do this, we count the number of occurrences in all possible combinations of the two categorical variables and plot those in individual plots. To investigate the associativity between the two categorical variables, we use the [Pearson Chi Squared](https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test) test. It is a statistical test applied to sets of categorical data to evaluate how likely it is that any observed difference between the sets arose by chance. It follows the general assumption, that we expect no change in the relative observed values if there is no association at all. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Picking Focus\n",
    "---\n",
    "In datasets as bis as the ones given, it is not meaningful to associate every attribute to every attribute. Instead, we want to investigate which attributes the most statistically associated to attributes that are a measure for the *Road Safety*.\n",
    "Following this train of though, the most important metric is the `Accident Severity`, which we can find in the `accidents` dataset at column index `6`. It makes sense to investigate, which attributes results in the most severe accident (ie. is most associated to the `Accident Severity`)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Linking Sub-Datasets\n",
    "---\n",
    "Since we also want to investigate, which `vehicle` and `casualty` attributes are most related to the `Accident Severity`, we need to prepare our data before actually associating. Namely: For each reported casualty (data record in `casualties`) and vehicle (data record in `vehicles`), we need to find the corresponding accident and its severity and map it to the specific casualty or vehicle. The following function `link_to_accidents` performs exactly this computation when called with `focus='Accident_Severity'`, as it returns a column vector of length of the number of vehicles/ casualty (in general: data.shape[0] (number of rows)), which contains the `Accident Severity` for each vehicle/ casualty. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_to_accidents(data, focus):\n",
    "    ans = []\n",
    "    for i in range(data.shape[0]):\n",
    "        index = data['Accident_Index'].iloc[i]\n",
    "        mask = DATA_LEEDS['accidents']['Accident_Index'] == index\n",
    "        focus_column = DATA_LEEDS['accidents'][mask][focus]\n",
    "        ans.append(int(focus_column))\n",
    "    return np.array(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEVERITY = {}\n",
    "for dataset in TABLENAMES:\n",
    "    SEVERITY[dataset] = link_to_accidents(DATA_LEEDS[dataset], focus='Accident_Severity')"
   ]
  },
  {
   "source": [
    "## Saving all Associations\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "for dataset in TABLENAMES:\n",
    "    save_all_categorical_scatters(data = DATA_LEEDS[dataset], summary = SUMMARY[dataset], severity = SEVERITY[dataset], path=PATH['reports']['leeds'] + PATH[dataset] + 'associations/')\n",
    "    save_all_categorical_associations(data = DATA_LEEDS[dataset], severity_summary = SUMMARY['accidents'], dataset_name = dataset, summary=SUMMARY[dataset], severity= SEVERITY[dataset], path=PATH['reports']['leeds'] + PATH[dataset] + 'associations/')"
   ]
  },
  {
   "source": [
    "## Report: Association between `Accident` and `Vehicle/ Casualty` Attribute \n",
    "---\n",
    "`TASK 2` specifically asks us to show one association between either an accident and a vehicle or an accident and a casualty attribute. Since we have associated the `Accident Severity` to each numerical and categorical attribute in all datasets, we will present the most intesting association here:\n",
    "\n",
    "**`Accident Severity / Sex of Driver`**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, V = categorical_association_test(DATA_LEEDS['vehicles'], SUMMARY['accidents'][6], SEVERITY['vehicles'], SUMMARY['vehicles'][14], DATA_LEEDS['vehicles'].iloc[:,14]);"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Spatial Visualisation (TASK 3)\n",
    "---\n",
    "So far, we have completely ignored the `geographic attributes` that were recorded for each accident in `accidents`. We will change this now by plotting Leed's accidents on a map to get a good visual intuition on where the accidents happen. To do this, we use a combination of function, namely `map_accidents` that under the hood calls `plot_point`. The functions are based on `folium` - an external Python package - that was earlier introduced and makes use of leaflet.js, which is a JavaScript package used to create interactive maps. \n",
    "After export, can be explored as an `html` file in the browser or inline within Jupyter. The map will contain a heat_map (if function argument `heat_map` = `True`), and for an arbitary of column in `accidens` as an argument for `focus` will make a visual distinctions for different values of this column."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Plot all Accidents in Leeds onto Map + Save Map Visualisation\n",
    "---\n",
    "Through the below cell, we save all maps generated with focuses on different attributes into the directory `../reports/leeds/maps`. We can study them in further detail by opening the `html` files in the browser.\n",
    "\n",
    "**REMARK**: Note that the execution of the below cell may take some time, since two maps (each ~3MB) need to be rendered."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "for boolean in [True, False]:\n",
    "    _map = map_accidents(DATA_LEEDS['accidents'], SUMMARY['accidents'], centroid=[53.8008, -1.5491], colors=['black', 'red', 'green'], heat_map=True, marker_cluster=boolean, focus='Accident_Severity')\n",
    "    save_map(_map, PATH['reports']['leeds'] + 'maps/', filename=f\"Accident_Severity_Map_(marker={boolean})\")"
   ]
  },
  {
   "source": [
    "## Report: Severity Map\n",
    "---\n",
    "TASK 2 specifically asks us to visualise the accidents on the map of Leeds, and color-code the plotted accident by their severity. This corresponds to the map saved at the path `../reports/leeds/maps/Accident_Severity_Map(marker=True)` ([Open here](http://127.0.0.1:5500/reports/leeds/maps/6_Accident_Severity.html)). \n",
    "The below cell shows how to manually plot this map, using the `map_accidents` function. We also specify the color-coding, such that 'black'=Fatal, 'red'=Serious and 'green'=Slight."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "severity_map = map_accidents(DATA_LEEDS['accidents'], SUMMARY['accidents'], centroid=[53.8008, -1.5491], colors=['black', 'red', 'green'], heat_map=True, marker_cluster=False, focus='Accident_Severity')\n",
    "severity_map"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Bike Safety in Leeds (TASK 4)\n",
    "---\n",
    "The final `TASK 4` asks us to investigate a self-chosen research question. As mentioned in the introduction to this Jupyter, our goal will be to make Leeds safer and more attractive to travel by bike. In this section we will develop...\n",
    "> 1) ...why bikes are important both for a city's road safety as well as environnment\n",
    "\n",
    "> 2) ...why Leeds is not a bike-city yet\n",
    "\n",
    "> 3) ...what measurements should be taken (based on the data) to make Leeds safer for bikes (and thereby making biking more attractive)\n",
    "\n",
    "https://ecf.com/news-and-events/news/data-collection-basis-better-road-safety"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Influence of Bikes on Road Safety and Environment\n",
    "---\n",
    "Promoting the use of cycling, walking, and public transportation as a means to navigate the city is part of the city of Leeds’ plan to achieve being carbon neutral by 2030. The potential benefits of embracing, and advocating for, the widespread use of these means of transportation also extends towards increased road safety; as witnessed by the city of Oslo which reported only 1 traffic related fatality in 2019 as a result of their extensive work to make the city more bike-friendly and improve overall traffic safety. While Leeds has experienced a reduction in road casualties in recent years, the annual number of people killed or seriously injured has remained largely unchanged.\n",
    "\n",
    "\n",
    "This report essentially acts to supplement the work already being done by the West Yorkshire Combined Authority through CityConnect, by investigating what factors significantly contribute to road accidents that involve cyclists, and how these might be addressed to effectively follow in Oslo’s bike tracks. The aim being to highlight any contributing factors that may be significant but unobvious, or identify accident hot-spots within Leeds that could potentially be redeveloped in the future."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2. Leeds' Traffic Flow \n",
    "---\n",
    "Before investigating the data, it is important to get a feel for how the traffic in Leeds looks currently. We have therefore used an external dataset, that reports the annual `Road Flow` in Leeds for different types of vehicles from 2000-2019. \n",
    "The dataset was obtained from: https://roadtraffic.dft.gov.uk/local-authorities/63 on 22.02.2021 (10AM) and is located in `..data/external`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Loading, Inspection and Processing of Dataset\n",
    "---\n",
    "We first load the external dataset using `pd.read_csv()` and inspect it roughly using pandas `.head()` method on `pd.DataFrame`. We get a first impression of the dataset: The dataset is already filtered for Leeds (as can be seen in `local_authority_name`) and contains the counts of different vehicle types that passed specific count points at different times. We can use this data to get a rough intuition on how the distribution of vehicles on Leeds' roads looks like:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_EXTERNAL['road_flow'] = pd.read_csv(PATH['data']['external'] + FILENAME['road_flow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_EXTERNAL['road_flow'].head()"
   ]
  },
  {
   "source": [
    "For our analysis we don't need the majority of columns. We drop them here."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_EXTERNAL['road_flow'].drop(columns=['count_point_id', 'direction_of_travel', 'count_date', 'hour', 'region_id', 'region_name', 'local_authority_id', 'local_authority_name', 'road_name', 'road_type', 'start_junction_road_name', 'end_junction_road_name', 'easting', 'northing', 'latitude', 'longitude','link_length_km', 'link_length_miles','hgvs_2_rigid_axle', 'hgvs_3_rigid_axle', 'hgvs_3_or_4_articulated_axle', 'hgvs_5_articulated_axle', 'hgvs_6_articulated_axle'], inplace=True)"
   ]
  },
  {
   "source": [
    "### Visualising Annual Road Flow\n",
    "---\n",
    "To visualise the annual `Road Flow` we need to count the occurrences of each vehicle type in different years. We do this by creating a mask for each year, and use this mask to count the occurences for each vehicle type. We save this information in a central data structure as `road_flow`. We then use standard code using `matplotlib` to create the figure that is based on the counts."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "road_flow = {}\n",
    "for year in np.unique(DATA_EXTERNAL['road_flow']['year']):\n",
    "    mask = DATA_EXTERNAL['road_flow']['year'] == year\n",
    "\n",
    "    try: road_flow['years'].append(year)\n",
    "    except: road_flow['years'] = [year]\n",
    "\n",
    "    for i in range(1,DATA_EXTERNAL['road_flow'].shape[1]-1):\n",
    "        try: road_flow[list(DATA_EXTERNAL['road_flow'])[i]].append(sum(DATA_EXTERNAL['road_flow'].iloc[:,i][mask]))\n",
    "        except: road_flow[list(DATA_EXTERNAL['road_flow'])[i]] = [sum(DATA_EXTERNAL['road_flow'].iloc[:,i][mask])]"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure and axes (with padding for better exporting)\n",
    "fig = plt.figure(figsize=(16,9))\n",
    "ax = fig.add_axes([.15,.15,.7,.7])    \n",
    "\n",
    "years = road_flow['years']\n",
    "labels = list(DATA_EXTERNAL['road_flow'])[1:-1]\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    ax.plot(years, road_flow[labels[i]], 'o-', label=labels[i])\n",
    "\n",
    "ax.set_xticks(years); ax.set_xticklabels(years);\n",
    "ax.set_title('Annual Leeds Traffic Flow by Vehicle (2000-2019)', fontweight='bold', fontsize=16)\n",
    "ax.set_xlabel('Years'); ax.set_ylabel('No. of Vehicles Counted');\n",
    "ax.get_yaxis().set_major_formatter(\n",
    "    matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "ax.legend();\n",
    "\n",
    "save_figure(fig, path=PATH['reports']['external'], filename='annual_leeds_traffic_flow', save_to='pdf')"
   ]
  },
  {
   "source": [
    "> SUMMARY: Leeds is a car-dominated city. There are hardly any bikes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3. Analysis\n",
    "---\n",
    "Due to its lack of a metro and few governmental initiatives of the government in the last few years, Leeds traffic is still highly dominated by motorised vehicles, making it dangerous for bikers. In the following section, we want to analyse specifically those accidents that involve bikes, and of each accident try to conclude patterns in the vehicle or casualty attributes of either the biker or the driver of the motorised vehicle. Some leading questions will be:\n",
    "> 1. `Accident Participants`: What patterns of accident participants can we observe? \n",
    "\n",
    "> 2. `Single Variable Analysis`: \n",
    "\n",
    ">> a. Under what accident circumstances do accidents happen that involve bikes? \n",
    "\n",
    ">> b. What vehicle and casualty attributes can be observed for particispants of accidents that involve bikes? (Filtered SVA)\n",
    "\n",
    "> 3. `Associations`: Which (accident/ casualty/ vehicle) attributes lead to more severe accidents involving bikes?\n",
    "\n",
    "> 4. `Spatial Visualisation`: How can we visualise the above on an informative maps?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Preparation of Analysis \n",
    "---\n",
    "Before however, analysing the above five points, we need to prepare our data in order to match our research question. Obviously, we need to reduce our datasets to only contain information about accidents that involves bikes in Leeds. To do so, we filter in the `vehicles` dataset for `Vehicle_Type` `1`, which corresponds to Pedal Cycles. We use this filtered vehicles dataset to get all unique `Accident Indexes` bikes were involved in. \n",
    "\n",
    "We can then conveniently iterate over all datasets to obtain the three datasets all reduced to only contain those accident that involved at least 1 bike. We safe this in a new dictionary called `DATA_LEEDS_BIKES` with the key of the identifier of the dataset and the additional key `all`. In contrast to the previous overall Leeds analysis we need this differentiation, because we will later filter in the sub-datasets `vehicles` and `casualties` for only bikers and motorised vehicles. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes = DATA_LEEDS['vehicles'][DATA_LEEDS['vehicles']['Vehicle_Type'] == 1]\n",
    "bike_accidents_indexes = set(bikes['Accident_Index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    DATA_LEEDS_BIKES[dataset] = {}\n",
    "    DATA_LEEDS_BIKES[dataset]['all'] = DATA_LEEDS[dataset][DATA_LEEDS[dataset]['Accident_Index'].isin(bike_accidents_indexes)]"
   ]
  },
  {
   "source": [
    "For our later analysis, we also want to prepare our two sub-datasets such that each of them is filtered to only contain information on the bikers and the other only on the motorised vehicle involved in the accident. We do this in the following few cells. We want our result to be saved in `DATA_LEEDS_BIKES[<sub-dataset_name]['bikers']` and `DATA_LEEDS_BIKES[<sub-dataset_name]['motorised']` respectively. We start by masking bikers in our reduced `DATA_LEEDS_BIKES['vehicles']` dataset and motorised vehicles through the inverse of this mask (a small look into the unique participants revealed that no pedestrian were involved in the recorded bike accidents, thus all accident participants that are not bikes, is a motorised vehicle)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masking bikers and non-bikers\n",
    "bikers = DATA_LEEDS_BIKES['vehicles']['all']['Vehicle_Type'] == 1 # \n",
    "motorised_vehicles = ~bikers\n",
    "\n",
    "DATA_LEEDS_BIKES['vehicles']['bikers'] = DATA_LEEDS_BIKES['vehicles']['all'][bikers]\n",
    "DATA_LEEDS_BIKES['vehicles']['motorised'] = DATA_LEEDS_BIKES['vehicles']['all'][motorised_vehicles]"
   ]
  },
  {
   "source": [
    "Now, however we also need to separate the involved casualties of accident involving bikes into bikers and drivers of motorised vehicles. To do this, we create a mask using `link_vehicles_to_casualties`, which iterates over all `casualties` to get their accident indx and casualty reference. We use this information to mask in `DATA_LEEDS_BIKES['vehicles']['bikers']` for this specific accident and append `True` if the casualty references match, otherwise `False`. This will create a `boolean array` that we can use to filter both for biker and motorised vehicle driver casualties (using its inverse)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_vehicles_to_casualties():\n",
    "    ans = []\n",
    "    for i in range(DATA_LEEDS_BIKES['casualties']['all'].shape[0]): # rows\n",
    "        index = DATA_LEEDS_BIKES['casualties']['all']['Accident_Index'].iloc[i] \n",
    "        cas_ref = DATA_LEEDS_BIKES['casualties']['all']['Vehicle_Reference'].iloc[i]\n",
    "\n",
    "        mask = DATA_LEEDS_BIKES['vehicles']['bikers']['Accident_Index'] == index\n",
    "        vehicles = DATA_LEEDS_BIKES['vehicles']['bikers'][mask]\n",
    "        for j in range(vehicles.shape[0]):\n",
    "            if vehicles['Vehicle_Reference'].iloc[j] == cas_ref:\n",
    "                ans.append(True)\n",
    "            else: ans.append(False)\n",
    "    return np.array(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS_BIKES['casualties']['bikers'] = DATA_LEEDS_BIKES['casualties']['all'][link_vehicles_to_casualties()]\n",
    "DATA_LEEDS_BIKES['casualties']['motorised'] = DATA_LEEDS_BIKES['casualties']['all'][~link_vehicles_to_casualties()]"
   ]
  },
  {
   "source": [
    "### Inspection of Datasets\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    print('-'*35)\n",
    "    for sub_dataset in ['all', 'bikers', 'motorised']:\n",
    "        try: print(f\"{dataset.capitalize()} ({sub_dataset[:1].capitalize()})\\t\\t{DATA_LEEDS_BIKES[dataset][sub_dataset].shape}\")\n",
    "        except: None"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Accident Vehicle Pattern\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = {}\n",
    "for index in DATA_LEEDS_BIKES['accidents']['all']['Accident_Index']:\n",
    "    mask = DATA_LEEDS_BIKES['vehicles']['all']['Accident_Index'] == index\n",
    "    vehicles = DATA_LEEDS_BIKES['vehicles']['all'][mask]['Vehicle_Type']\n",
    "    combination = sorted(tuple(vehicles))\n",
    "\n",
    "    try: combinations[str(combination)] += 1\n",
    "    except: combinations[str(combination)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure and axes (with padding for better exporting)\n",
    "fig = plt.figure(figsize=(16,9))\n",
    "ax = fig.add_axes([.15,.15,.7,.7]) # [left, bottom, width, height]\n",
    "\n",
    "# plot \n",
    "x, y = list(combinations.keys()), list(combinations.values())\n",
    "ax.barh(x, y, align='center')\n",
    "ax.set_title('Accident Pattern for Bike Accidents', fontweight='bold')\n",
    "ax.set_ylabel('Combinations of Accident Participants'); ax.set_xlabel('Number of Accidents (2019)')\n",
    "\n",
    "# insert counts and percentages as text next to the corresponding bars\n",
    "for x_cord, y_cord in zip(x,y):\n",
    "    ax.text(y_cord, x_cord, f'{y_cord} ({str(100*round(y_cord/sum(y), 3))[:5]}%)' , color='black')\n",
    "\n",
    "save_figure(fig, path=PATH['reports']['bikes'] + 'all/', filename='participants_pattern')"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Single-Variable Analysis\n",
    "---\n",
    "We have sucessfullly constrainted our datasets for bike accidents in Leeds. In a total of 238 accidents in 2019, 480 vehicles (out of which 238 were bikes and 242 were motorised vehicles) were involved in the accidents. 236 out of 240 total reported casualties were bikers, while only 4 were drivers of motorised vehicles. With these reduced datasets, we want to further investigate into the circumstances of the accidents, as well as patterns in the casualties and vehicles of both the bikers and drivers of motorised vehicles. \n",
    "\n",
    "We start our single-variable analysis by developing another summary datastructure that we need for plotting efficiently. We will call this datastructure `SUMMARY_BIKES` and it will be structured in the same way as `DATA_LEEDS_BIKES` is, namely with the three sub-dictionaries `all`, `bikes` and `motorised`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### a. Overall Bike-Accidents\n",
    "\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_BIKES = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    SUMMARY_BIKES[dataset] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, start_at in zip(TABLENAMES, [2, 37, 22]): # start_at corresponds to sheet indexes at which it switches to new dataset\n",
    "    initialise_summary(\n",
    "        data = DATA_LEEDS_BIKES[dataset]['all'], \n",
    "        lookup = VARIABLE_LOOKUP, \n",
    "        dataset_name = dataset, \n",
    "        key = 'all', \n",
    "        summary = SUMMARY_BIKES[dataset], \n",
    "        labels = LABELS[dataset], \n",
    "        plotting = PLOTTING[dataset], \n",
    "        fivenum = FIVENUM[dataset], \n",
    "        start_at=start_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time mapping\n",
    "SUMMARY_BIKES['accidents']['all'][11]['Map'] = {i: f\"{i}-{i+1}\" for i in range(24)}\n",
    "SUMMARY_BIKES['accidents']['all'][11]['Map'][-1] = 'N/A'\n",
    "\n",
    "# date mapping\n",
    "months = ['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC']\n",
    "SUMMARY_BIKES['accidents']['all'][9]['Map'] = {i+1: months[i] for i in range(12)}\n",
    "\n",
    "# no. casualties and vehicles\n",
    "SUMMARY_BIKES['accidents']['all'][7]['Map'] = {i: i for i in np.unique(DATA_LEEDS['accidents']['Number_of_Vehicles'])}\n",
    "SUMMARY_BIKES['accidents']['all'][8]['Map'] = {i: i for i in np.unique(DATA_LEEDS['accidents']['Number_of_Casualties'])}\n",
    "\n",
    "# fixed bug for windows file naming convention (no question marks allowed)\n",
    "SUMMARY_BIKES['vehicles']['all'][12]['Name'] = 'Was_Vehicle_Left_Hand_Drive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    compute_numerical_summary(SUMMARY_BIKES[dataset]['all'], DATA_LEEDS_BIKES[dataset]['all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    save_numerical_report(SUMMARY_BIKES[dataset]['all'], path=PATH['reports']['bikes'] + 'all/' + PATH[dataset] + 'numerical_summary/', filename=dataset, save_to='csv')\n",
    "    save_numerical_report(SUMMARY_BIKES[dataset]['all'], path=PATH['reports']['bikes'] + 'all/' + PATH[dataset] + 'numerical_summary/', filename=dataset, save_to='json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "for dataset in TABLENAMES:\n",
    "    save_all_single_variable_analysis(SUMMARY_BIKES[dataset]['all'], path=PATH['reports']['bikes'] + 'all/' + PATH[dataset] + 'single_variable_analysis/', missing_values=False)"
   ]
  },
  {
   "source": [
    "#### b. Analysis of Bikers and Motor Vehicle Driver\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in TABLENAMES:\n",
    "    SUMMARY_BIKES[dataset]['bikers'] = {}\n",
    "    SUMMARY_BIKES[dataset]['motorised'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, start_at in zip(TABLENAMES[1:], [37, 22]): # start_at corresponds to sheet indexes at which it switches to new dataset\n",
    "    initialise_summary(\n",
    "        data = DATA_LEEDS_BIKES[dataset]['all'], \n",
    "        lookup = VARIABLE_LOOKUP, \n",
    "        dataset_name = dataset, \n",
    "        key = 'bikers', \n",
    "        summary = SUMMARY_BIKES[dataset], \n",
    "        labels = LABELS[dataset], \n",
    "        plotting = PLOTTING[dataset], \n",
    "        fivenum = FIVENUM[dataset], \n",
    "        start_at=start_at)\n",
    "    initialise_summary(\n",
    "        data = DATA_LEEDS_BIKES[dataset]['all'], \n",
    "        lookup = VARIABLE_LOOKUP, \n",
    "        dataset_name = dataset, \n",
    "        key = 'motorised', \n",
    "        summary = SUMMARY_BIKES[dataset], \n",
    "        labels = LABELS[dataset], \n",
    "        plotting = PLOTTING[dataset], \n",
    "        fivenum = FIVENUM[dataset], \n",
    "        start_at=start_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    if dataset != 'accidents':\n",
    "        for sub_dataset in ['bikers', 'motorised']:\n",
    "            compute_numerical_summary(SUMMARY_BIKES[dataset][sub_dataset], DATA_LEEDS_BIKES[dataset][sub_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    if dataset != 'accidents':\n",
    "        for sub_dataset in ['bikers', 'motorised']:\n",
    "            save_numerical_report(SUMMARY_BIKES[dataset][sub_dataset], path=PATH['reports']['bikes'] + f'{sub_dataset}/' + PATH[dataset] + 'numerical_summary/', filename=dataset, save_to='csv')\n",
    "            save_numerical_report(SUMMARY_BIKES[dataset][sub_dataset], path=PATH['reports']['bikes'] + f'{sub_dataset}/' + PATH[dataset] + 'numerical_summary/', filename=dataset, save_to='json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "for dataset in TABLENAMES:\n",
    "    if dataset != 'accidents':\n",
    "        for sub_dataset in ['bikers', 'motorised']:\n",
    "            save_all_single_variable_analysis(SUMMARY_BIKES[dataset][sub_dataset], path=PATH['reports']['bikes'] + f'{sub_dataset}/' + PATH[dataset] + 'single_variable_analysis/', missing_values=False)"
   ]
  },
  {
   "source": [
    "#### c. Findings\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Most Common Bike Accident\n",
    "94.2% of the bicycle accidents occur with cars or taxis, and in almost all of those cases the bicyclist gets injured. In contrast,only four drivers of motorised vehicles are being injured in all recorded bike accidents. By looking at where these accidentsoccur, it can be seen that in most cases the accidents are at junctions, where the cyclist is going straight and the car is turning,and the collision is front to front. This suggests that in most bicycle accident scenarios the car driver turns into the bicycliston accident. This recurring pattern of accident participants, manoeuvres, and location of accidents can and should be used toactively prevent these patterns of accident scenarios from happening. This could ie. be achieved through bike-specific trafficlights, longer delay periods after red lights or similar measures."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_accident_junction_location = barplot(SUMMARY_BIKES['accidents']['all'][18], keep_missing_values=False)\n",
    "vehicle_manouvre_bike = barplot(SUMMARY_BIKES['vehicles']['bikers'][4], keep_missing_values=False)\n",
    "vehicle_manouvre_motorised_vehicle = barplot(SUMMARY_BIKES['vehicles']['motorised'][4], keep_missing_values=False)\n",
    "first_point_of_impact_bike = barplot(SUMMARY_BIKES['vehicles']['bikers'][11], keep_missing_values=False)\n",
    "first_point_of_impact_motorised_vehicle = barplot(SUMMARY_BIKES['vehicles']['motorised'][11], keep_missing_values=False)"
   ]
  },
  {
   "source": [
    "### On the Way to Work\n",
    "The data gives various arguments for the fact that most bike accidents happen during the rush hour times from 6-9 and 15-19.Firstly, most of the participants involved in bike accidents stated to be on the way to or from their workplace. This can also beseen in the distribution of accidents in different times of the day. The road collision numbers peak during the rush hours. Thedistribution of bike accidents during the week and the year further strengthen the above argument. On days of the weekendsthere occur noticeably less accidents when compared to regular week days. In the distribution of bike accidents per month, itis visible that the number of bike collisions drop in the month of August, which is the month of national summer holiday inthe UK. Therefore less people commute to or from work, and thus the number of bike accidents d\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_accidents_purpose = barplot(SUMMARY_BIKES['vehicles']['all'][13],  keep_missing_values=False) # purpose\n",
    "bike_accidents_time = barplot(SUMMARY_BIKES['accidents']['all'][11], keep_missing_values=False) # time of the day\n",
    "bike_accidents_day = barplot(SUMMARY_BIKES['accidents']['all'][10], keep_missing_values=False) # day of the week\n",
    "bike_accidents_month = barplot(SUMMARY_BIKES['accidents']['all'][9], keep_missing_values=False) # month of the year"
   ]
  },
  {
   "source": [
    "### Hazardous, Big Roads\n",
    "Most of the accidents happen on streets classified as class A, which in the UK refers to main carriageways with high speedand little bends. The three most hazardous roads are the roads with road number 660 (Woodhouse Lane or Otley Road),65 (Kirkstall Road) and 61 (Regent Street). Together over 15% of all accidents involving bikes occur on these streets.Naturally, these roads have more accidents happening since they are more trafficked throughout the year than less centralroads. However, the high percentage of bike accidents happening at the three mentioned road, gives reason to believe that theroads are poorly secured for safe bike-travel and should therefore be a focus of political measurements trying to reduce thenumber of bike accidents in Lee"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_class = barplot(SUMMARY_BIKES['accidents']['all'][14], keep_missing_values=False)\n",
    "road_number = barplot(SUMMARY_BIKES['accidents']['all'][15], keep_missing_values=False)"
   ]
  },
  {
   "source": [
    "### 3. Associations\n",
    "---\n",
    "We first associate all variabels that make sense to associate, in order to find the factor influencing the severity and frequency of accidents involving bikes the most. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEVERITY_BIKES = {}\n",
    "for dataset in TABLENAMES:\n",
    "    SEVERITY_BIKES[dataset] = link_to_accidents(DATA_LEEDS_BIKES[dataset]['all'], focus='Accident_Severity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "for dataset in TABLENAMES:\n",
    "    save_all_categorical_scatters(data = DATA_LEEDS_BIKES[dataset]['all'], summary = SUMMARY_BIKES[dataset]['all'], severity=SEVERITY_BIKES[dataset], path=PATH['reports']['bikes'] + 'all/' + PATH[dataset] + 'associations/')\n",
    "    save_all_categorical_associations(data = DATA_LEEDS_BIKES[dataset]['all'], severity_summary = SUMMARY['accidents'], dataset_name = dataset, summary=SUMMARY_BIKES[dataset]['all'], severity= SEVERITY_BIKES[dataset], path=PATH['reports']['bikes'] + 'all/' + PATH[dataset] + 'associations/')"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 4. Spatial Visualisation\n",
    "---\n",
    "Finally, we would like to have a nice geospatial representation of where our bike accidents happened. With the following cell we generate maps that color-code the plotted accidents depending on different variables using the same code as in section `TASK 3`. All maps are exported in `html` format and can be viewed when opening in a browser of choice. The maps are saved in the directory `../reports/bikes/maps`.\n",
    "\n",
    "**REMARK**: Note that the execution of the below cell may take some time, since several maps (each ~3MB) need to be rendered."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "for boolean in [True, False]:\n",
    "    _map = map_accidents(DATA_LEEDS_BIKES['accidents']['all'], SUMMARY_BIKES['accidents']['all'], centroid=[53.8008, -1.5491], heat_map=True, marker_cluster=boolean, focus='Accident_Severity')\n",
    "    save_map(_map, PATH['reports']['bikes'] + 'maps/', filename=f\"Accident_Severity_Map_(marker={boolean})\")"
   ]
  },
  {
   "source": [
    "### Bikes Accidents in Leeds (Focus on Accident Severity)\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_severity = map_accidents(DATA_LEEDS_BIKES['accidents']['all'], SUMMARY_BIKES['accidents']['all'], centroid=[53.8008, -1.5491], marker_cluster=True, focus='Accident_Severity')\n",
    "bike_severity"
   ]
  }
 ]
}